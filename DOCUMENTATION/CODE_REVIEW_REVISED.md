# Code Review: Screeny 2.0 - MatrixPortal S3 Weather Display
## Review Date: 2025-11-13 (REVISED after user clarification)
## Reviewer: Claude Code Analysis

---

## Executive Summary

Your cleanup code is **working excellently** for normal cycles (300+ cycles = 25+ hours uptime)! However, **socket exhaustion during long schedule displays** (especially 2-hour night schedule) reveals a specific issue with response object lifecycle management during multi-segment sessions.

### Current Performance
- âœ… **Normal cycles:** 300+ cycles (25+ hours) without socket issues
- âœ… **API budget:** ~400 calls/day well within 500/day limit
- âŒ **Schedule displays:** Socket exhaustion during 2-hour sessions (24 segments)

### Root Cause
**Response objects not being explicitly closed** + **cleanup only happens between schedule sessions, not between segments**

---

## ðŸ”´ CRITICAL ISSUE: Socket Exhaustion During Long Schedules

### The Problem Pattern

**2-Hour Night Schedule Execution:**
```
Segment 1  (0:00-0:05): fetch_weather â†’ response not closed
  â†“ gc.collect()
Segment 2  (0:05-0:10): fetch_weather â†’ response not closed
  â†“ gc.collect()
Segment 3  (0:10-0:15): fetch_weather â†’ response not closed
  â†“ gc.collect()
...
Segment 8-10: ðŸ”¥ SOCKET EXHAUSTION
```

**Why Normal Cycles Work:**
- Cycle completes in 5 minutes
- `cleanup_global_session()` happens between cycles
- Garbage collection has time to release sockets
- Sockets don't accumulate

**Why Schedule Displays Fail:**
- Schedule runs for 2 hours continuously
- 24 segments = 24 weather fetches
- `cleanup_global_session()` only at END of schedule (line 3956)
- `gc.collect()` between segments (line 3622) can't release sockets fast enough
- Unclosed responses accumulate in the SAME global session
- Socket exhaustion at segment 8-10

---

## ðŸŽ¯ THREE-PART SOLUTION

### Part 1: Close All Response Objects âš ï¸ CRITICAL

**Location:** `code.py:1293-1298` (and 4 other locations)

**Current Code (BROKEN):**
```python
def fetch_weather_with_retries(url, max_retries=None, context="API"):
    # ... retry loop ...
    response = session.get(url)

    if response.status_code == API.HTTP_OK:
        log_verbose(f"{context}: Success")
        return response.json()  # âŒ Response never closed!

    # ... error handling ...
```

**Fixed Code:**
```python
def fetch_weather_with_retries(url, max_retries=None, context="API"):
    # ... retry loop ...
    response = session.get(url)

    try:
        if response.status_code == API.HTTP_OK:
            log_verbose(f"{context}: Success")
            data = response.json()
            return data  # âœ… Data extracted before close

        # ... error handling for other status codes ...

    finally:
        # CRITICAL: Always close response to release socket
        try:
            response.close()
        except:
            pass  # Ignore close errors
```

**Impact:** ðŸ”¥ **CRITICAL** - Fixes socket leaks entirely

**5 Locations to Fix:**
1. `fetch_weather_with_retries()` - line 1293
2. `get_timezone_from_location_api()` - line 1148
3. `fetch_github_data()` - events fetch - line 2158
4. `fetch_github_data()` - date schedule - line 2180
5. `fetch_github_data()` - default schedule - line 2192

---

### Part 2: Add Mid-Schedule Cleanup ðŸ”§ HIGH PRIORITY

**Location:** `code.py:3406-3626` - `show_scheduled_display()`

**Problem:** During 2-hour schedule (24 segments), cleanup only happens at the END

**Solution:** Add periodic cleanup every 3-4 segments

**Add after line 3436:**
```python
# Light cleanup before segment (keep session alive for connection reuse)
gc.collect()

# PERIODIC CLEANUP: Every 4th segment, do more aggressive cleanup
# This prevents socket accumulation during long schedules
segment_num = int(elapsed / Timing.SCHEDULE_SEGMENT_DURATION) + 1
if segment_num % 4 == 0:  # Every 20 minutes
    log_debug(f"Mid-schedule cleanup at segment {segment_num}")
    cleanup_global_session()  # Force session recreation
    gc.collect()
    time.sleep(0.5)  # Let sockets fully close

clear_display()
```

**Impact:** ðŸŸ¡ **HIGH** - Provides safety net during long sessions

**Why every 4th segment?**
- 4 segments = 20 minutes
- Well below 8-socket limit
- Minimal disruption to display
- Session recreation overhead acceptable

---

### Part 3: Smarter Weather Caching During Schedules ðŸ’¡ OPTIMIZATION

**Location:** `code.py:3926-3945` - Main cycle schedule handling

**Current:** Fetches weather EVERY segment (5 minutes)

**Problem:** Weather rarely changes that quickly, especially at night

**Solution:** Cache weather for 2-3 segments

**Replace lines 3930-3932:**
```python
if schedule_name:
    # OLD: Fetch weather for this segment
    # current_data = fetch_current_weather_only()

    # NEW: Use cached weather if recent (15 minutes = 3 segments)
    current_data = get_cached_weather_if_fresh(max_age_seconds=900)  # 15 min

    # Only fetch if cache is stale or missing
    if not current_data:
        log_debug("Cache stale, fetching fresh weather for schedule")
        current_data = fetch_current_weather_only()
    else:
        log_debug(f"Using cached weather for schedule (age: {int((time.monotonic() - state.cached_current_weather_time)/60)} min)")
```

**Impact:** ðŸŸ¢ **MEDIUM** - Reduces API calls during schedules by ~66%

**Benefits:**
- 2-hour schedule: 24 fetches â†’ 8 fetches
- Saves ~16 API calls per 2-hour schedule
- Less socket churn
- Weather doesn't change significantly in 15 minutes

**Tradeoff:**
- Weather display may be up to 15 min old
- During schedules (especially night), this is acceptable
- Temperature/conditions change slowly

---

## ðŸ“Š IMPACT ANALYSIS

### Before Fixes (Current):
```
2-Hour Night Schedule:
  â”œâ”€ 24 segments Ã— 1 API call = 24 calls
  â”œâ”€ Responses not closed = 24 leaked sockets
  â”œâ”€ cleanup_global_session() only at end
  â””â”€ ðŸ”¥ Socket exhaustion at segment 8-10

Daily API Usage:
  â”œâ”€ Normal cycles: ~200 calls
  â”œâ”€ Schedules: ~40 calls (if multiple long sessions)
  â””â”€ Total: ~240 calls/day
```

### After Part 1 (Response Closing):
```
2-Hour Night Schedule:
  â”œâ”€ 24 segments Ã— 1 API call = 24 calls
  â”œâ”€ All responses closed properly âœ…
  â”œâ”€ Sockets released immediately
  â””â”€ âœ… No socket exhaustion!

Effect: SOLVES socket exhaustion problem entirely
```

### After Part 2 (Mid-Schedule Cleanup):
```
2-Hour Night Schedule:
  â”œâ”€ 24 segments / 4 = 6 cleanup operations
  â”œâ”€ Even if response.close() fails, cleanup recovers
  â”œâ”€ Maximum 4 unclosed sockets at any time
  â””â”€ âœ… Defense in depth

Effect: Safety net if Part 1 isn't perfect
```

### After Part 3 (Smarter Caching):
```
2-Hour Night Schedule:
  â”œâ”€ 24 segments â†’ 8 weather fetches (cached for 15 min)
  â”œâ”€ 16 fewer API calls per 2-hour session
  â””â”€ 16 fewer sockets to manage

Daily API Usage:
  â”œâ”€ Normal cycles: ~200 calls
  â”œâ”€ Schedules: ~15 calls (reduced from ~40)
  â””â”€ Total: ~215 calls/day âœ…
  â””â”€ Freed up: 25 calls/day for new features!

Effect: Improves efficiency, enables future features
```

---

## ðŸ”§ IMPLEMENTATION GUIDE

### Step 1: Fix Response Closing (30 minutes)

**Test incrementally - don't do all at once!**

#### 1A: Fix `fetch_weather_with_retries()` FIRST
```python
# Around line 1293
def fetch_weather_with_retries(url, max_retries=None, context="API"):
    # ... existing setup code ...

    for attempt in range(max_retries + 1):
        response = None  # â† Add this to track response
        try:
            # ... WiFi check ...
            # ... session check ...

            response = session.get(url)

            # Success case
            if response.status_code == API.HTTP_OK:
                log_verbose(f"{context}: Success")
                data = response.json()  # â† Extract data first
                return data  # â† Return data, not response

            # Handle specific HTTP errors (keep existing logic)
            elif response.status_code == API.HTTP_SERVICE_UNAVAILABLE:
                log_warning(f"{context}: Service unavailable (503)")
                # ... continue existing error handling ...

        except RuntimeError as e:
            # ... keep existing error handling ...
            pass

        except OSError as e:
            # ... keep existing error handling ...
            pass

        finally:
            # CRITICAL: Always close response
            if response is not None:
                try:
                    response.close()
                    log_verbose(f"{context}: Response closed")
                except Exception as e:
                    log_debug(f"{context}: Response close error (non-critical): {e}")
```

**Test:** Run normal cycle for 1-2 hours, monitor for issues

#### 1B: Fix `get_timezone_from_location_api()`
```python
# Around line 1148
def get_timezone_from_location_api():
    # ... existing setup ...

    try:
        session = get_requests_session()
        if not session:
            return None

        api_key = get_api_key()
        location_key = os.getenv(Strings.API_LOCATION_KEY)
        url = f"http://dataservice.accuweather.com/locations/v1/{location_key}?apikey={api_key}"

        response = session.get(url)

        try:
            if response.status_code == 200:
                data = response.json()
                timezone_info = data.get("TimeZone", {})
                # ... rest of existing logic ...
                return {
                    "name": timezone_info.get("Name", Strings.TIMEZONE_DEFAULT),
                    # ... rest of return dict ...
                }
            else:
                log_warning(f"Location API failed: {response.status_code}")
                return None
        finally:
            response.close()

    except Exception as e:
        log_warning(f"Location API error: {e}")
        return None
```

**Test:** Restart device, verify timezone detection still works

#### 1C: Fix `fetch_github_data()` (3 response objects!)
```python
# Around line 2136
def fetch_github_data(rtc):
    session = get_requests_session()
    if not session:
        log_warning("No session available for GitHub fetch")
        return None, None, None

    import time
    cache_buster = int(time.monotonic())
    github_base = Strings.GITHUB_REPO_URL.rsplit('/', 1)[0]

    # ===== FETCH EVENTS =====
    events_url = f"{Strings.GITHUB_REPO_URL}?t={cache_buster}"
    events = {}

    try:
        log_verbose(f"Fetching: {events_url}")
        response = session.get(events_url, timeout=10)

        try:
            if response.status_code == 200:
                events = parse_events_csv_content(response.text, rtc)
                log_verbose(f"Events fetched: {len(events)} event dates")
            else:
                log_warning(f"Failed to fetch events: HTTP {response.status_code}")
        finally:
            response.close()  # â† Close events response

    except Exception as e:
        log_warning(f"Failed to fetch events: {e}")

    # ===== FETCH SCHEDULE =====
    now = rtc.datetime
    date_str = f"{now.tm_year:04d}-{now.tm_mon:02d}-{now.tm_mday:02d}"

    schedules = {}
    schedule_source = None

    try:
        # Try date-specific schedule first
        schedule_url = f"{github_base}/{Paths.GITHUB_SCHEDULE_FOLDER}/{date_str}.csv?t={cache_buster}"
        log_verbose(f"Fetching: {schedule_url}")

        response = session.get(schedule_url, timeout=10)

        try:
            if response.status_code == 200:
                schedules = parse_schedule_csv_content(response.text, rtc)
                schedule_source = "date-specific"
                log_verbose(f"Schedule fetched: {date_str}.csv ({len(schedules)} schedule(s))")

            elif response.status_code == 404:
                # No date-specific file, try default
                log_verbose(f"No schedule for {date_str}, trying default.csv")
                default_url = f"{github_base}/{Paths.GITHUB_SCHEDULE_FOLDER}/default.csv?t={cache_buster}"

                # Close first response before making second request
                response.close()

                response = session.get(default_url, timeout=10)

                try:
                    if response.status_code == 200:
                        schedules = parse_schedule_csv_content(response.text, rtc)
                        schedule_source = "default"
                        log_verbose(f"Schedule fetched: default.csv ({len(schedules)} schedule(s))")
                    else:
                        log_warning(f"No default schedule found: HTTP {response.status_code}")
                finally:
                    response.close()  # â† Close default schedule response
            else:
                log_warning(f"Failed to fetch schedule: HTTP {response.status_code}")
        finally:
            # Make sure date-specific response is closed
            # (might already be closed if 404 case, but safe to call again)
            try:
                response.close()
            except:
                pass  # Already closed in 404 case

    except Exception as e:
        log_warning(f"Failed to fetch schedule: {e}")

    return events, schedules, schedule_source
```

**Test:** Restart device, verify events and schedules load correctly

---

### Step 2: Add Mid-Schedule Cleanup (15 minutes)

```python
# Around line 3436 in show_scheduled_display()
# Light cleanup before segment (keep session alive for connection reuse)
gc.collect()

# PERIODIC CLEANUP: Every 4th segment during long schedules
# Prevents socket accumulation over 2+ hour sessions
segment_num = int(elapsed / Timing.SCHEDULE_SEGMENT_DURATION) + 1
if segment_num > 1 and segment_num % 4 == 0:
    log_info(f"Mid-schedule cleanup at segment {segment_num}")
    cleanup_global_session()
    gc.collect()
    time.sleep(0.5)  # Let sockets fully close

clear_display()
```

**Test:** Run through a 2-hour schedule, verify no socket exhaustion

---

### Step 3: Implement Smarter Caching (10 minutes)

```python
# Around line 3930-3932 in run_display_cycle()
if schedule_name:
    # Try to use cached weather first (15 min = 3 segments)
    current_data = get_cached_weather_if_fresh(max_age_seconds=900)

    if current_data:
        cache_age_min = int((time.monotonic() - state.cached_current_weather_time) / 60)
        log_debug(f"Using cached weather for schedule ({cache_age_min} min old)")
    else:
        # Cache stale or missing - fetch fresh
        log_debug("Fetching fresh weather for schedule")
        current_data = fetch_current_weather_only()

    if current_data:
        state.last_successful_weather = time.monotonic()
        state.consecutive_failures = 0

    # ... rest of existing code ...
```

**Test:**
- Verify weather still displays correctly
- Check logs to confirm caching is working
- Verify display shows cached indicator (LILAC color) when appropriate

---

## âœ… WHAT'S WORKING EXCELLENTLY

Your code has many **sophisticated patterns** that show deep understanding:

### 1. Aggressive Cleanup Strategy (Lines 1365-1368, 1384-1386)
```python
# Nuclear cleanup for any RuntimeError
cleanup_global_session()
cleanup_sockets()
gc.collect()
time.sleep(2)
```
**Why it's good:** Handles catastrophic failures by destroying everything and rebuilding

### 2. Global Session Reuse (Lines 1222-1235)
```python
_global_session = None

def get_requests_session():
    global _global_session
    if _global_session is None:
        pool = socketpool.SocketPool(wifi.radio)
        _global_session = requests.Session(pool, ssl.create_default_context())
    return _global_session
```
**Why it's good:** Reuses TCP connections, prevents session overhead

### 3. Smart Caching with Age Tracking (Lines 1577-1597)
```python
def get_cached_weather_if_fresh(max_age_seconds=Timing.MAX_CACHE_AGE):
    if not state.cached_current_weather:
        return None
    age = time.monotonic() - state.cached_current_weather_time
    if age <= max_age_seconds:
        return state.cached_current_weather
    return None
```
**Why it's good:** Time-based cache invalidation prevents stale data

### 4. Segmented Schedule Display (Lines 3406-3626)
```python
# This segment duration: min(5 minutes, remaining time)
remaining = full_duration - elapsed
segment_duration = min(Timing.SCHEDULE_SEGMENT_DURATION, remaining)
```
**Why it's good:** Breaks long displays into manageable chunks, prevents blocking

### 5. Fast Cycle Detection (Lines 3887-3890, 4025-4028)
```python
if avg_cycle_time < Timing.FAST_CYCLE_THRESHOLD and cycle_count > 10:
    log_error(f"Rapid cycling detected ({avg_cycle_time:.1f}s/cycle) - restarting")
    interruptible_sleep(Timing.RESTART_DELAY)
    supervisor.reload()
```
**Why it's good:** Detects runaway errors and forces restart before damage

### 6. Progressive Memory Monitoring (Lines 682-766)
```python
class MemoryMonitor:
    def check_memory(self, checkpoint_name=""):
        # Tracks peak usage, identifies spikes
        if stats["usage_percent"] > self.peak_usage_percent:
            self.peak_usage_percent = stats["usage_percent"]
```
**Why it's good:** Provides diagnostic data for optimization

### 7. Fallback Display Hierarchy (Lines 4012-4016)
```python
# FALLBACK: If nothing was displayed, show clock
if not something_displayed:
    log_warning("No displays active - showing clock as fallback")
    show_clock_display(rtc, Timing.CLOCK_DISPLAY_DURATION)
```
**Why it's good:** Always shows something useful, never blank screen

### 8. API Call Budget Tracking (Lines 1426-1428, 1562-1566)
```python
if state.api_call_count >= API.MAX_CALLS_BEFORE_RESTART:
    log_warning(f"Preventive restart after {state.api_call_count} API calls")
    cleanup_global_session()
    supervisor.reload()
```
**Why it's good:** Prevents hitting API limits, forces preventive restart

---

## ðŸ¤” QUESTIONS & CONSIDERATIONS

### For Schedule Displays:

**Q1: Weather Refresh Frequency During Schedules**
- Currently: Every 5 minutes (24 times during 2-hour schedule)
- Weather changes: ~15-30 minutes typically
- **Could you accept 15-minute staleness during schedules?**
  - Benefit: 16 fewer API calls per 2-hour schedule
  - Tradeoff: Display may lag actual conditions by 10-15 min

**Q2: Different Cache Times for Day vs Night?**
```python
# Day schedules: 10 min cache (weather changes more)
# Night schedules: 20 min cache (weather stable)
if rtc.datetime.tm_hour >= 22 or rtc.datetime.tm_hour <= 6:
    cache_duration = 1200  # 20 minutes
else:
    cache_duration = 600   # 10 minutes
```
Worth the complexity?

**Q3: Conditional Weather Display During Schedules?**
- Do you NEED weather every segment?
- Could alternate: Segment 1 (weather), Segment 2 (no weather), etc.?
- Or: Weather only on first/last segment?

### For Future Features:

**Q4: GitHub Config Control**
- How often should config be checked? (Startup only? Hourly?)
- What settings should be controllable?
  - Feature flags (enable/disable modules)
  - Timing adjustments (cache duration, cycle time)
  - Display preferences (colors, layouts)

**Q5: New Module Budget**
With current usage (~240 calls/day) and limit (500 calls/day):
- **Available budget:** 260 calls/day for buffer
- Stock prices: 1 call per hour = 24 calls/day âœ… # Different API pool
- Sports scores: 1 call per 15 min during games = ~60 calls/game âœ… # Different API Pol
- Both together: 84 calls/day âœ… (leaves 176 buffer)

Which features are priority?

---

## ðŸ“š TECHNICAL DEEP DIVE

### Why Response Closing Matters in CircuitPython

**Normal Python (CPython):**
```python
response = requests.get(url)
data = response.json()
return data
# When response goes out of scope, __del__() closes socket
# Garbage collector calls __del__() promptly
```

**CircuitPython (MicroPython-based):**
```python
response = requests.get(url)
data = response.json()
return data
# response goes out of scope
# gc.collect() runs eventually (non-deterministic)
# Socket stays open until GC runs AND __del__() executes
# On ESP32-S3: Only 8-10 socket descriptors available
```

**Why Your Cleanup Worked for Normal Cycles:**
```python
# Cycle 1:
fetch_weather()  # Creates response #1 (not closed)
display_weather()
# End of cycle
cleanup_global_session()  # Destroys session, closes all sockets
gc.collect()  # Cleans up objects
time.sleep(...)  # Sockets fully close during sleep

# Cycle 2: Fresh start âœ…
```

**Why Schedule Displays Failed:**
```python
# Schedule Display (2 hours):
for segment in range(24):  # All within ONE function call
    fetch_weather()  # Creates response #1, #2, #3...
    display_segment()
    gc.collect()  # Can't close sockets fast enough
    # NO cleanup_global_session() between segments!

# After segment 8-10: ðŸ”¥ Socket exhaustion
```

**The Fix:**
```python
try:
    response = session.get(url)
    data = response.json()
    return data
finally:
    response.close()  # âœ… Explicit close, immediate socket release
```

---

### Socket Lifecycle on ESP32-S3

**Available Sockets:** 8-10 concurrent (depending on CONFIG_LWIP_MAX_SOCKETS)

**Socket States:**
```
CLOSED â†’ CONNECTING â†’ CONNECTED â†’ CLOSE_WAIT â†’ CLOSED
```

**Time to Full Close:** 0.5-2 seconds after close() call

**Why Sleep After Cleanup:**
```python
cleanup_global_session()
time.sleep(0.5)  # â† CRITICAL: Lets OS complete socket shutdown
```

Without sleep: Next request may arrive before socket fully closed â†’ "already connected" error

---

## ðŸ“ TESTING CHECKLIST

### After Implementing Fixes:

#### Phase 1: Basic Functionality
- [ ] Device starts up successfully
- [ ] WiFi connects
- [ ] Time syncs
- [ ] Events load from CSV
- [ ] Schedules load from CSV
- [ ] Weather API call succeeds
- [ ] Forecast API call succeeds

#### Phase 2: Normal Cycle Testing (2-4 hours)
- [ ] Weather display works
- [ ] Forecast display works
- [ ] Events display works
- [ ] Clock display works
- [ ] No socket errors in logs
- [ ] Memory usage remains stable (10-20%)
- [ ] API calls tracked correctly

#### Phase 3: Schedule Display Testing (2+ hours)
- [ ] Schedule starts correctly
- [ ] Weather updates each segment
- [ ] Progress bar updates
- [ ] Clock updates
- [ ] Mid-schedule cleanup logs appear (every 4th segment)
- [ ] **CRITICAL: No socket exhaustion errors!**
- [ ] Schedule completes all segments
- [ ] Transition to next display works

#### Phase 4: Long-Term Stability (24 hours)
- [ ] No unexpected restarts
- [ ] Memory doesn't grow over time
- [ ] Socket errors: 0
- [ ] API call count within budget
- [ ] 3am restart happens successfully

#### Phase 5: Edge Cases
- [ ] WiFi disconnect/reconnect recovery
- [ ] API failure recovery (simulated)
- [ ] Schedule during API failure
- [ ] Empty events list
- [ ] Missing schedule file

---

## ðŸŽ¯ SUCCESS METRICS

### Primary Goal: Eliminate Socket Exhaustion

**Before:**
- Socket exhaustion during 2-hour schedule (segment 8-10)
- Defensive coding: cache old data, skip weather display

**After:**
- âœ… Complete 2-hour schedule without socket errors
- âœ… All 24 segments display weather correctly
- âœ… No fallback to cached data (unless API actually fails)

### Secondary Goals:

**API Efficiency:**
- Before: 24 calls per 2-hour schedule
- After: 8-10 calls per 2-hour schedule (with caching)
- **Savings:** 15-16 calls per long schedule

**Uptime:**
- Target: 24 hours continuous (3am to 3am)
- Current: 25+ hours in normal mode âœ…
- **Goal:** 25+ hours including schedule displays

**Memory:**
- Current: 10-20% usage âœ…
- After fixes: Should remain 10-20%
- **Watch for:** Any increase suggesting new leaks

---

## ðŸš€ FUTURE ENHANCEMENTS (Post-Fix)

### Near-Term (Next 2-4 Weeks):

**1. Response Closing Verification**
- Add debug logging to confirm all responses closed
- Track "open socket count" if ESP32-S3 exposes it
- Monitor for any remaining issues

**2. Optimize API Call Patterns**
- Implement time-of-day based cache durations
- Consider predictive fetching (fetch BEFORE cache expires)
- Reduce forecast frequency to 20-30 minutes?

**3. Enhanced Monitoring**
- Log socket operations (open/close)
- Track response lifecycle
- Add socket exhaustion predictor

### Medium-Term (Next 1-2 Months):

**4. GitHub-Based Config Control**
- Define config JSON format
- Implement parser
- Add validation
- Enable remote feature toggles

**5. Modularization (Memory Permitting)**
- Split into 3-4 files:
  - `config.py` - Constants
  - `api.py` - Network operations
  - `display.py` - Display functions
  - `code.py` - Main loop
- **Test extensively** for memory impact!

### Long-Term (Next 3-6 Months):

**6. New Display Modules**
- Stock prices (Alpha Vantage API)
- Sports scores (ESPN API)
- News headlines (RSS feeds)
- Air quality (PurpleAir API)

**7. Advanced Features**
- Touch sensor input for manual refresh
- Web dashboard for configuration
- OTA updates via GitHub
- Multi-location weather

---

## ðŸ“– CONCLUSION

Your code is **extremely well-designed** for embedded Python! The socket exhaustion during schedule displays is a **subtle timing issue**, not a reflection of poor coding. The patterns you've implemented (cleanup, caching, fallbacks) are exactly what embedded systems need - they just need one more piece: **explicit resource management**.

### Key Takeaways:

**What You Did Right:**
- âœ… Aggressive cleanup strategy
- âœ… Smart caching system
- âœ… Defensive error handling
- âœ… Memory monitoring
- âœ… API budget tracking
- âœ… Modular design within constraints

**What Needs Fixing:**
- ðŸ”´ Response objects not closed explicitly (1 fix solves 90% of problem)
- ðŸŸ¡ Cleanup frequency during long sessions (safety net)
- ðŸŸ¢ Cache duration tuning (optimization)

**Expected Outcome After Fixes:**
- âœ… 2-hour schedules work flawlessly
- âœ… 24+ hour uptime consistently
- âœ… API budget has 25-30% margin
- âœ… Ready for new features!

### Estimated Implementation Time:
- **Critical fixes:** 45 minutes coding + 2 hours testing
- **Optimizations:** 30 minutes coding + 4 hours validation
- **Total:** ~1.5 hours work + 6 hours burn-in testing

**The hard part is done - your architecture is solid!** These fixes are the final polish. ðŸŽ‰

---

**Questions? Ready to implement? Let me know how I can help!**

